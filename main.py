import logging
from transformers import AutoModelForCausalLM, AutoTokenizer

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")

def load_model(model_name="gpt2"):
    """
    Load a pre-trained model and tokenizer.
    :param model_name: Hugging Face model name
    :return: tokenizer and model
    """
    logging.info(f"Loading model: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)

    # Set pad_token to eos_token to avoid padding issues
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    return tokenizer, model

def generate_text(prompt, tokenizer, model, max_length=50, temperature=0.7, top_k=50):
    """
    Generate text based on the given prompt.
    :param prompt: Input text to the model
    :param tokenizer: Tokenizer for the model
    :param model: Pre-trained model
    :param max_length: Maximum length of generated text
    :param temperature: Sampling temperature; higher values generate more random text
    :param top_k: Limits the sampling to the top-k tokens
    :return: Generated text
    """
    logging.info("Preparing input for the model...")
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)

    logging.info("Generating text...")
    outputs = model.generate(
        inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_length=max_length,
        pad_token_id=tokenizer.eos_token_id,  # Explicitly set pad token ID
        temperature=temperature,             # Adjust temperature for creativity
        top_k=top_k,                         # Use top-k sampling
        do_sample=True,                      # Enable sampling for varied output
        num_return_sequences=1
    )
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    logging.info("Text generation complete.")
    return generated_text

def save_to_file(prompt, generated_text, file_path="generated_texts.txt"):
    """
    Save the input prompt and generated text to a file.
    :param prompt: The input prompt given to the model
    :param generated_text: The text generated by the model
    :param file_path: Path to the file where data will be saved
    """
    with open(file_path, "a") as file:
        file.write(f"Prompt: {prompt}\n")
        file.write(f"Generated Text: {generated_text}\n")
        file.write("-" * 80 + "\n")
    logging.info(f"Output saved to {file_path}")

if __name__ == "__main__":
    logging.info("Initializing the text generator...")
    tokenizer, model = load_model()

    print("Enter a prompt to generate text:")
    prompt = input("> ")

    print("\nGenerating text...\n")
    generated_text = generate_text(prompt, tokenizer, model)
    print(generated_text)

    # Save prompt and generated text to a file
    save_to_file(prompt, generated_text)
